{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f72455a-a7c0-4222-b3f7-bb16d167a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import LSTM, Dense  \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader, TensorDataset  \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from transformers import BertTokenizer, BertModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b467a91-4208-4198-bc9e-62b807221fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('data/开发大表_修订.csv')\n",
    "\n",
    "# chunk_size = 1000  # 设置每次读取的行数\n",
    "# dataframes = pd.read_csv('data/汇总数据_修订_天数对齐.csv', chunksize=chunk_size)\n",
    "# df2 = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b87e69-8172-482b-a372-efdf4fb69ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 根据'井号'进行一对多生成新的数据  \n",
    "# df = pd.merge(df2, df1, on='井号', how='inner')\n",
    "# columns = df.columns\n",
    "# new_columns = []\n",
    "# for column in columns:\n",
    "#     new_columns.append(column.replace('\\n', '').replace(' ', ''))\n",
    "# df.columns = new_columns\n",
    "# df.to_csv('data/数据ALL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27eaf36b-7c63-4374-bd49-ebc6cba2781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 1000  # 设置每次读取的行数\n",
    "# dataframes = pd.read_csv('data/数据ALL.csv', chunksize=chunk_size)\n",
    "# df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b8d5f1-c63f-41dc-94cf-e23ed1a38889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['日产水','日外输气量','历年累计产水量','备注说明', '泡排施工日期','泡排当前日期','泡排施工前日产（万方/天）','90天累产(104m3)',\n",
    "#         '泡排施工后日产（万方/天）','泡排备注','设备规格',\n",
    "#          '作业井号', '施工日期', '施工开始时间', '施工结束时间',\n",
    "#        '施工排量（方/小时）', '气举方式', '施工前压力（兆帕）', '施工最高压力（兆帕）',\n",
    "#        '施工后压力（兆帕）', '施工前瞬量（万方/天）', '施工最高瞬产（万方/天）'\n",
    "#         ,'施工后瞬量（万方/天）','注气量（万方）'\n",
    "#         ,'施工前日产（万方）', '施工后日产（万方）','排液量（方）','作业时间（小时）','备注','考虑天数'\n",
    "#         ,'增产气量(万方）','气举类型','施工时段','产气量','实际产气量','是否增压','增压机设备名称','增压机类型','增压机备注'\n",
    "#         ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1fb8b2-d334-46a0-8b3b-3b89ac1b5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 指定列名，将含有空值的特定列填充为0\n",
    "# null_columns = df.columns[df.isna().any()].tolist()\n",
    "# for null_column in null_columns:\n",
    "#     df[null_column].fillna(0, inplace=True)\n",
    "# df.to_csv('data/数据ALL_修订.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba8d173-a68e-4ab3-aa31-d611b9658e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000  # 设置每次读取的行数\n",
    "dataframes = pd.read_csv('data/数据ALL_修订.csv', chunksize=chunk_size)\n",
    "df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178a8e9c-3cfe-404d-a87e-4c5a855a607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['井号', '日期', '生产时间', '套压', '油压', '日产气', '输压', '历年累计产气量', '甜点分区', '生产天数',\n",
       "       '研究所复核储量丰度-2023.6.30(108m3/km2)', 'I类储层厚度(m)', 'I类储层连续厚度(m)',\n",
       "       '龙一11厚度(m)', '龙一11(1类厚度)(m)', '龙一11水平长度(m)', '龙一11压力系数', '龙一11钻遇率(%)',\n",
       "       '龙一11底以上4米箱体钻遇率（%）', '水平段长(m)', '主体单段簇数', '单段主体孔数', '簇间距(m)',\n",
       "       '主体射孔密度(P/m)', '加砂强度(t/m)', '用液强度(m3/m)', '分段段长(m)', '是否套变', '合压长度(m)',\n",
       "       '丢段长度(m)', '优质页岩钻遇率（%）', '设计压裂段长(m)', '实际射孔簇数', '折算有效段数', '100目石英砂(t)',\n",
       "       '40/70目石英砂(t)', '40/70目陶粒(t)', '30/50目陶粒(t)', '总砂量(t)', '酸液(m3)',\n",
       "       '滑溜水(m3)', '线性胶(m3)', '弱凝胶(m3)', '总液量(m3)', '平均单段砂量(t)', '平均单段液量(m3)',\n",
       "       '陶粒比例（%）', '平均停泵压力(MPa)', '排量（m3/min）', '压裂结束停泵压力(MPa)', '焖井时间(天)',\n",
       "       '开始排液时井口套压(MPa)', '投产前累计排液量(m3)', '投产前返排率(%)', '井间距(m)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d467b4-2aaa-4f26-add6-193108898f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['井号'] == '威202H1-1'  ]\n",
    "# df1.drop(['日期','日产气','甜点分区','井号'], axis=1, inplace=True)\n",
    "# df1.drop(['日期','井号'], axis=1, inplace=True)\n",
    "data = df1[['历年累计产气量']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5b2787-c5b1-4bcc-9fb0-0f408186607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m data\u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data)  \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m----> 9\u001b[0m \u001b[43maa\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 创建数据集  \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(data, seq_length):  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "data = df1['历年累计产气量'].values\n",
    "data = data.reshape(-1, data.shape[-1])  \n",
    "\n",
    "# 数据归一化  \n",
    "scaler = MinMaxScaler()  \n",
    "data= scaler.fit_transform(data)  \n",
    "\n",
    "print(len(data))\n",
    "aa\n",
    "# 创建数据集  \n",
    "def create_dataset(data, seq_length):  \n",
    "    X, y = [], []  \n",
    "    for i in range(len(data) - seq_length):  \n",
    "        X.append(data[i:(i + seq_length), :])  \n",
    "        y.append(data[(i + seq_length), :])  \n",
    "    return np.array(X), np.array(y)  \n",
    "\n",
    "\n",
    "# 创建数据集  \n",
    "seq_length = 10  # 序列长度，根据你的数据调整  \n",
    "X, y = create_dataset(data, seq_length)  \n",
    "\n",
    "print(X)\n",
    "aa\n",
    "\n",
    "# 转换为PyTorch TensorDataset  \n",
    "tensor_x = torch.from_numpy(X)  \n",
    "tensor_y = torch.from_numpy(y)  \n",
    "dataset = TensorDataset(tensor_x, tensor_y)  \n",
    "  \n",
    "# 创建DataLoader  \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  \n",
    "  \n",
    "# 加载预训练的Transformer模型和tokenizer  \n",
    "model_name = 'bert-base-uncased'  # 使用BERT作为示例，你可以选择其他预训练模型，如'bert-base-chinese'、'bert-large-cased'等  \n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)  \n",
    "model = BertModel.from_pretrained(model_name)  \n",
    "  \n",
    "# 为模型的输出层添加一个新的线性层，以适应你的任务（多分类或回归）  \n",
    "if len(data.shape) == 2:  # 如果数据是二维的（即，有多个特征），则使用多分类问题。假设有num_classes个类。  \n",
    "    num_classes = 10  # 假设有10个类，根据你的任务调整。  \n",
    "    model.classifier = Linear(model.config.hidden_size, num_classes)  \n",
    "else:  # 如果数据是一维的，则使用回归问题。  \n",
    "    model.classifier = Linear(model.config.hidden_size, 1)  \n",
    "  \n",
    "# 定义损失函数和优化器  \n",
    "criterion = MSELoss()  # 使用均方误差作为损失函数，适用于回归问题。对于多分类问题，你可能想使用CrossEntropyLoss。  \n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # 使用Adam优化器。你可以根据你的需要更改学习率。  \n",
    "  \n",
    "# 训练模型  \n",
    "num_epochs = 100  # 训练轮数，根据你的需要调整。  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 检查是否有可用的GPU，并将模型和数据移动到适当的设备上。  \n",
    "model.to(device)  # 将模型移动到设备上。如果你的设备是CPU，则不需要此行。  \n",
    "for epoch in range(num_epochs):  \n",
    "    model.train()  # 设置模型为训练模式。  \n",
    "    for batch in dataloader:  \n",
    "        input_ids = batch[0].to(device)  # 将输入数据传递到设备（例如GPU）中。如果你的设备是CPU，则不需要此行。  \n",
    "        target = batch[1].to(device)  # 将目标数据传递到设备中。如果你的设备是CPU，则不需要此行。  \n",
    "        outputs = model(input_ids)  # 前向传播，得到预测输出。  \n",
    "        loss = criterion(outputs, target)  # 计算损失。  \n",
    "        optimizer.zero_grad()  # 清零梯度。  \n",
    "        loss.backward()  # 反向传播。  \n",
    "        optimizer.step()  # 更新权重。  \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")  # 打印每个epoch的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed603c4-af93-4b0b-8195-4039f6a50ba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m历年累计产气量\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[i \u001b[38;5;241m+\u001b[39m n_steps])  \n\u001b[0;32m      8\u001b[0m X, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)  \n\u001b[1;32m----> 9\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, n_features))  \u001b[38;5;66;03m# 输入数据的形状：(样本数, 时间步长, 特征数)  \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 创建LSTM模型  \u001b[39;00m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()  \n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# 构建输入数据形状  \n",
    "n_features = 1#len(featureNames)  # 单个输入特征（时间序列数据）  \n",
    "n_steps = 3  # 输入序列的步长（考虑过去的X个时间步）  \n",
    "X, y = [], []  \n",
    "for i in range(len(data) - n_steps):  \n",
    "    X.append(data['历年累计产气量'].values[i:i + n_steps])  \n",
    "    y.append(data['历年累计产气量'].values[i + n_steps])  \n",
    "X, y = np.array(X), np.array(y)  \n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))  # 输入数据的形状：(样本数, 时间步长, 特征数)  \n",
    "  \n",
    "# 创建LSTM模型  \n",
    "model = Sequential()  \n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))  # 50个隐藏单元  \n",
    "model.add(Dense(1))  # 输出层，使用线性激活函数  \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')  # 使用均方误差作为损失函数，使用Adam优化器进行训练  \n",
    "  \n",
    "# 训练模型  \n",
    "model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2)  # 使用80%的数据进行训练，20%的数据用于验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa66e6b-b515-459d-9f14-8999e31ed65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd62d6-563f-4169-b06f-280f7775d7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec72f7-d6be-4086-b8e4-50f93484b9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
